{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AsMFKCALr3y"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/07-lcel.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PQji83qLpVC"
   },
   "source": [
    "#### LangChain Essentials Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22dLBWVZLpVD"
   },
   "source": [
    "# LangChains Expression Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiRUqmfBLpVE"
   },
   "source": [
    "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
    "\n",
    "In this example, we will introduce LangChain's Expression Langauge (LCEL), abstracting a full chain and understanding how it will work. We'll provide examples for both OpenAI's `gpt-4o-mini` *and* Meta's `llama3.2` via Ollama!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "i690tIabLwb_"
   },
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#   langchain-core==0.3.33 \\\n",
    "#   langchain-openai==0.3.3 \\\n",
    "#   langchain-community==0.3.16 \\\n",
    "#   langsmith==0.3.4 \\\n",
    "#   docarray==0.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOR87lEpLpVE"
   },
   "source": [
    "---\n",
    "\n",
    "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfpD-R6sLpVE"
   },
   "source": [
    "---\n",
    "\n",
    "> ⚠️ If using LangSmith, add your API key below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "exBqQHgqLpVE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
    "    getpass(\"Enter LangSmith API Key: \")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-lcel-openai\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYGeByKjLpVF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQbgErgdLpVF"
   },
   "source": [
    "## Traditional Chains vs LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dyluns_ELpVF"
   },
   "source": [
    "In this section we're going to dive into a basic example using the traditional method for building chains before jumping into LCEL. We will build a pipeline where the user must input a specific topic, and then the LLM will look and return a report on the specified topic. Generating a _research report_ for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTZTTiK7LpVF"
   },
   "source": [
    "### Traditional LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDHfbKZ7LpVF"
   },
   "source": [
    "The `LLMChain` is the simplest chain originally introduced in LangChain. This chain takes a prompt, feeds it into an LLM, and _optionally_ adds an output parsing step before returning the result.\n",
    "\n",
    "Let's see how we construct this using the traditional method, for this we need:\n",
    "\n",
    "* `prompt` — a `PromptTemplate` that will be used to generate the prompt for the LLM.\n",
    "* `llm` — the LLM we will be using to generate the output.\n",
    "* `output_parser` — an optional output parser that will be used to parse the structured output of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Jyz2vWLoLpVF"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"Give me a small report on {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rykro39YLpVF"
   },
   "source": [
    "For the LLM, we'll start by initializing our connection to the OpenAI API. We do need an OpenAI API key, which you can get from the [OpenAI platform](https://platform.openai.com/api-keys).\n",
    "\n",
    "We will use the `gpt-4o-mini` model with a `temperature` of `0.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Uvs7hILRLpVF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
    "    or getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhs6_3c4LpVF",
    "outputId": "b7051222-b74e-45c0-d19f-ac28a6bb87fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None}, id='run--6ad8cbcf-536b-4f32-90a3-270c6978414e-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = llm.invoke(\"Hello there\")\n",
    "llm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V39qjMpLLpVG"
   },
   "source": [
    "Then we define our output parser, this will be used to parse the output of the LLM. In this case, we will use the `StrOutputParser` which will parse the `AIMessage` output from our LLM into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "46qTjklnLpVG"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "z0uwxedBLpVG",
    "outputId": "faa83a90-aa81-40ba-b9a9-625507c7b7be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = output_parser.invoke(llm_out)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk4CUy52LpVG"
   },
   "source": [
    "Through the `LLMChain` class we can place each of our components into a linear `chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PrFk1J9dLpVG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/b59r49t147vcx84q80s5dl840000gn/T/ipykernel_62938/2960353250.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjNsVMVpLpVG"
   },
   "source": [
    "Note that the `LLMChain` _was_ deprecated in LangChain `0.1.17`, the expected way of constructing these chains today is through LCEL, which we'll cover in a moment.\n",
    "\n",
    "We can `invoke` our `chain`, providing a `topic` that we'd like to be researched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDDq8Cq7LpVG",
    "outputId": "e30de2aa-7005-45f4-cb96-e681e9c2c1d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'retrieval augmented generation',\n",
       " 'text': '### Report on Retrieval-Augmented Generation (RAG)\\n\\n#### Introduction\\nRetrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge bases or documents during the generation process, thereby improving the accuracy and relevance of the generated content.\\n\\n#### Key Concepts\\n1. **Information Retrieval**: RAG utilizes a retrieval mechanism to fetch relevant documents or snippets from a large corpus based on a given query. This step ensures that the model has access to up-to-date and contextually relevant information.\\n\\n2. **Natural Language Generation**: After retrieving relevant information, the model generates responses or text based on both the input query and the retrieved documents. This dual approach allows for more informed and contextually rich outputs.\\n\\n3. **Hybrid Architecture**: RAG typically employs a two-component architecture:\\n   - **Retriever**: This component identifies and retrieves relevant documents from a knowledge base or database.\\n   - **Generator**: This component processes the retrieved documents along with the original query to produce coherent and contextually appropriate text.\\n\\n#### Advantages\\n- **Enhanced Accuracy**: By leveraging external information, RAG can provide more accurate and factually correct responses, especially in domains where knowledge is constantly evolving.\\n- **Contextual Relevance**: The integration of retrieval allows the model to generate responses that are more relevant to the specific context of the query.\\n- **Scalability**: RAG can scale to large datasets, making it suitable for applications requiring extensive knowledge bases.\\n\\n#### Applications\\n- **Customer Support**: RAG can be used in chatbots to provide accurate answers to customer inquiries by retrieving relevant information from product manuals or FAQs.\\n- **Content Creation**: Writers can use RAG to generate articles or reports that are well-informed and factually accurate by pulling in data from various sources.\\n- **Research Assistance**: RAG can assist researchers by summarizing findings from multiple papers or articles, providing a comprehensive overview of a topic.\\n\\n#### Challenges\\n- **Quality of Retrieval**: The effectiveness of RAG heavily depends on the quality of the retrieval component. Poor retrieval can lead to irrelevant or incorrect information being used in the generation process.\\n- **Complexity**: Implementing a RAG system can be more complex than traditional generation models, requiring careful tuning of both the retrieval and generation components.\\n- **Latency**: The retrieval process can introduce latency, which may be a concern in real-time applications.\\n\\n#### Conclusion\\nRetrieval-Augmented Generation represents a significant advancement in the field of natural language processing. By combining retrieval and generation, RAG systems can produce more accurate, relevant, and context-aware outputs. As research and development in this area continue, RAG is likely to play an increasingly important role in various applications, from customer service to content generation and beyond. Future work will focus on improving retrieval accuracy, reducing latency, and enhancing the overall user experience.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbHqc1qLLpVG"
   },
   "source": [
    "We can view a formatted version of this output using the `Markdown` display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 980
    },
    "id": "hBxPHOZ-LpVG",
    "outputId": "23c54f43-dc98-4c56-a47f-4a996b206eb7"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Report on Retrieval-Augmented Generation (RAG)\n",
       "\n",
       "#### Introduction\n",
       "Retrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge bases or documents during the generation process, thereby improving the accuracy and relevance of the generated content.\n",
       "\n",
       "#### Key Concepts\n",
       "1. **Information Retrieval**: RAG utilizes a retrieval mechanism to fetch relevant documents or snippets from a large corpus based on a given query. This step ensures that the model has access to up-to-date and contextually relevant information.\n",
       "\n",
       "2. **Natural Language Generation**: After retrieving relevant information, the model generates responses or text based on both the input query and the retrieved documents. This dual approach allows for more informed and contextually rich outputs.\n",
       "\n",
       "3. **Hybrid Architecture**: RAG typically employs a two-component architecture:\n",
       "   - **Retriever**: This component identifies and retrieves relevant documents from a knowledge base or database.\n",
       "   - **Generator**: This component processes the retrieved documents along with the original query to produce coherent and contextually appropriate text.\n",
       "\n",
       "#### Advantages\n",
       "- **Enhanced Accuracy**: By leveraging external information, RAG can provide more accurate and factually correct responses, especially in domains where knowledge is constantly evolving.\n",
       "- **Contextual Relevance**: The integration of retrieval allows the model to generate responses that are more relevant to the specific context of the query.\n",
       "- **Scalability**: RAG can scale to large datasets, making it suitable for applications requiring extensive knowledge bases.\n",
       "\n",
       "#### Applications\n",
       "- **Customer Support**: RAG can be used in chatbots to provide accurate answers to customer inquiries by retrieving relevant information from product manuals or FAQs.\n",
       "- **Content Creation**: Writers can use RAG to generate articles or reports that are well-informed and factually accurate by pulling in data from various sources.\n",
       "- **Research Assistance**: RAG can assist researchers by summarizing findings from multiple papers or articles, providing a comprehensive overview of a topic.\n",
       "\n",
       "#### Challenges\n",
       "- **Quality of Retrieval**: The effectiveness of RAG heavily depends on the quality of the retrieval component. Poor retrieval can lead to irrelevant or incorrect information being used in the generation process.\n",
       "- **Complexity**: Implementing a RAG system can be more complex than traditional generation models, requiring careful tuning of both the retrieval and generation components.\n",
       "- **Latency**: The retrieval process can introduce latency, which may be a concern in real-time applications.\n",
       "\n",
       "#### Conclusion\n",
       "Retrieval-Augmented Generation represents a significant advancement in the field of natural language processing. By combining retrieval and generation, RAG systems can produce more accurate, relevant, and context-aware outputs. As research and development in this area continue, RAG is likely to play an increasingly important role in various applications, from customer service to content generation and beyond. Future work will focus on improving retrieval accuracy, reducing latency, and enhancing the overall user experience."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbmq0DfeLpVG"
   },
   "source": [
    "That is a simple `LLMChain` using the traditional LangChain method. Now let's move onto LCEL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWets8OpLpVG"
   },
   "source": [
    "## LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Lr5NA0xLpVG"
   },
   "source": [
    "**L**ang**C**hain **E**xpression **L**anguage (LCEL) is the recommended approach to building chains in LangChain. Having superceeded the traditional methods with `LLMChain`, etc. LCEL gives us a more flexible system for building chains. The pipe operator `|` is used by LCEL to _chain_ together components. Let's see how we'd construct an `LLMChain` using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8R-9b7ulLpVH"
   },
   "outputs": [],
   "source": [
    "lcel_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdJvzetzLpVH"
   },
   "source": [
    "We can `invoke` this chain in the same way as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "XKWzIwpCLpVH",
    "outputId": "9dc83fdd-98a4-4513-dcac-055092aeeb12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Report on Retrieval-Augmented Generation (RAG)\\n\\n#### Introduction\\nRetrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge bases or documents during the generation process, thereby improving the accuracy and relevance of the generated content.\\n\\n#### Key Concepts\\n1. **Retrieval Mechanism**: RAG employs a retrieval system to fetch relevant documents or pieces of information from a large corpus. This is typically done using techniques such as vector embeddings or traditional keyword-based search.\\n\\n2. **Generation Mechanism**: After retrieving relevant information, a generative model (often based on transformer architectures like GPT or BERT) synthesizes this information into coherent and contextually appropriate responses.\\n\\n3. **Hybrid Model**: RAG can be viewed as a hybrid model that integrates both retrieval and generation, allowing it to leverage vast amounts of external knowledge while maintaining the fluency and contextual understanding of generative models.\\n\\n#### Advantages\\n- **Enhanced Knowledge Access**: By retrieving information from external sources, RAG can provide more accurate and up-to-date responses compared to models that rely solely on pre-trained knowledge.\\n- **Contextual Relevance**: The integration of retrieval allows the model to generate responses that are more relevant to the specific query or context, improving user satisfaction.\\n- **Scalability**: RAG systems can scale to incorporate new information without the need for extensive retraining, making them adaptable to changing knowledge landscapes.\\n\\n#### Applications\\n- **Question Answering**: RAG is particularly effective in applications where users seek specific information, such as customer support or educational tools.\\n- **Content Creation**: It can assist in generating articles, summaries, or reports by pulling in relevant data from various sources.\\n- **Conversational Agents**: RAG enhances chatbots and virtual assistants by enabling them to provide informed responses based on real-time data retrieval.\\n\\n#### Challenges\\n- **Complexity**: The integration of retrieval and generation components can complicate the architecture and increase the computational resources required.\\n- **Quality of Retrieved Information**: The effectiveness of RAG heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to inaccurate or misleading outputs.\\n- **Latency**: The retrieval process can introduce latency, which may affect the responsiveness of applications, especially in real-time scenarios.\\n\\n#### Conclusion\\nRetrieval-Augmented Generation represents a significant advancement in the field of natural language processing, combining the strengths of retrieval systems with generative models. Its ability to access and utilize external knowledge makes it a powerful tool for a variety of applications, from question answering to content creation. However, challenges related to complexity, information quality, and latency must be addressed to fully realize its potential. As research and development in this area continue, RAG is poised to play a crucial role in the evolution of intelligent systems.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7E8_msHLpVH"
   },
   "source": [
    "The output format is slightly different, but the underlying functionality and content being output is the same. As before, we can view a formatted version of this output using the `Markdown` display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "id": "42uPzB1JLpVH",
    "outputId": "0d7254e1-0cfe-43ff-dea2-dfad2f760007"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Report on Retrieval-Augmented Generation (RAG)\n",
       "\n",
       "#### Introduction\n",
       "Retrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge bases or documents during the generation process, thereby improving the accuracy and relevance of the generated content.\n",
       "\n",
       "#### Key Concepts\n",
       "1. **Retrieval Mechanism**: RAG employs a retrieval system to fetch relevant documents or pieces of information from a large corpus. This is typically done using techniques such as vector embeddings or traditional keyword-based search.\n",
       "\n",
       "2. **Generation Mechanism**: After retrieving relevant information, a generative model (often based on transformer architectures like GPT or BERT) synthesizes this information into coherent and contextually appropriate responses.\n",
       "\n",
       "3. **Hybrid Model**: RAG can be viewed as a hybrid model that integrates both retrieval and generation, allowing it to leverage vast amounts of external knowledge while maintaining the fluency and contextual understanding of generative models.\n",
       "\n",
       "#### Advantages\n",
       "- **Enhanced Knowledge Access**: By retrieving information from external sources, RAG can provide more accurate and up-to-date responses compared to models that rely solely on pre-trained knowledge.\n",
       "- **Contextual Relevance**: The integration of retrieval allows the model to generate responses that are more relevant to the specific query or context, improving user satisfaction.\n",
       "- **Scalability**: RAG systems can scale to incorporate new information without the need for extensive retraining, making them adaptable to changing knowledge landscapes.\n",
       "\n",
       "#### Applications\n",
       "- **Question Answering**: RAG is particularly effective in applications where users seek specific information, such as customer support or educational tools.\n",
       "- **Content Creation**: It can assist in generating articles, summaries, or reports by pulling in relevant data from various sources.\n",
       "- **Conversational Agents**: RAG enhances chatbots and virtual assistants by enabling them to provide informed responses based on real-time data retrieval.\n",
       "\n",
       "#### Challenges\n",
       "- **Complexity**: The integration of retrieval and generation components can complicate the architecture and increase the computational resources required.\n",
       "- **Quality of Retrieved Information**: The effectiveness of RAG heavily depends on the quality and relevance of the retrieved documents. Poor retrieval can lead to inaccurate or misleading outputs.\n",
       "- **Latency**: The retrieval process can introduce latency, which may affect the responsiveness of applications, especially in real-time scenarios.\n",
       "\n",
       "#### Conclusion\n",
       "Retrieval-Augmented Generation represents a significant advancement in the field of natural language processing, combining the strengths of retrieval systems with generative models. Its ability to access and utilize external knowledge makes it a powerful tool for a variety of applications, from question answering to content creation. However, challenges related to complexity, information quality, and latency must be addressed to fully realize its potential. As research and development in this area continue, RAG is poised to play a crucial role in the evolution of intelligent systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbutFcpyLpVI"
   },
   "source": [
    "### How Does the Pipe Operator Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIjOY7VILpVI"
   },
   "source": [
    "Before moving onto other LCEL features, let's take a moment to understand what the pipe operator `|` is doing and _how_ it works.\n",
    "\n",
    "Functionality wise, the pipe tells you that whatever the _left_ side outputs will be fed as input into the _right_ side. In the example of `prompt | llm | output_parser`, we see that `prompt` feeds into `llm` feeds into `output_parser`.\n",
    "\n",
    "The pipe operator is a way of chaining together components, and is a way of saying that whatever the _left_ side outputs will be fed as input into the _right_ side.\n",
    "\n",
    "Let's make a basic class named `Runnable` that will transform our a provided function into a _runnable_ class that we will then use with the pipe `|` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9xTvwvF4LpVI"
   },
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other.invoke(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2Vg_PhXLpVI"
   },
   "source": [
    "With the `Runnable` class, we will be able wrap a function into the class, allowing us to then chain together multiple of these _runnable_ functions using the `__or__` method.\n",
    "\n",
    "First, let's create a few functions that we'll chain together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "G7HCw9-VLpVI"
   },
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    return x+5\n",
    "\n",
    "def sub_five(x):\n",
    "    return x-5\n",
    "\n",
    "def mul_five(x):\n",
    "    return x*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0fDvmtWLpVI"
   },
   "source": [
    "Now we wrap our functions with the `Runnable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LfnBBcbxLpVI"
   },
   "outputs": [],
   "source": [
    "add_five_runnable = Runnable(add_five)\n",
    "sub_five_runnable = Runnable(sub_five)\n",
    "mul_five_runnable = Runnable(mul_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jHLbl_7LpVI"
   },
   "source": [
    "Finally, we can chain these together using the `__or__` method from the `Runnable` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHFo61TwLpVI",
    "outputId": "3a03c07b-7ca9-45f5-ab2e-a979565b7e42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oim1zWHLpVI"
   },
   "source": [
    "So we can see that we're able to chain together our functions using `__or__`. The pipe `|` operator is simply a shortcut for the `__or__` method, so we can create the exact same chain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uh3-jF8pLpVJ",
    "outputId": "933ea922-f587-4f98-e825-b26113c5dc08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Lnj00T8LpVJ"
   },
   "source": [
    "## LCEL `RunnableLambda`\n",
    "\n",
    "The `RunnableLambda` class is LangChain's built-in method for constructing a _runnable_ object from a function. That is, it does the same thing as the custom `Runnable` class we created earlier. Let's try it out with the same functions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BmMH8GzVLpVJ"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_five_runnable = RunnableLambda(add_five)\n",
    "sub_five_runnable = RunnableLambda(sub_five)\n",
    "mul_five_runnable = RunnableLambda(mul_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkpwIKq6LpVJ"
   },
   "source": [
    "We chain these together again with the pipe `|` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9e58vaSELpVJ"
   },
   "outputs": [],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FdqbdAyLpVJ"
   },
   "source": [
    "And call them using the `invoke` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqHBMT8ULpVJ",
    "outputId": "b32dcaf1-1207-4cd4-f881-d9ffd5a754d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47mu4xxqLpVJ"
   },
   "source": [
    "Now we want to try something a little more testing, so this time we will generate a report, and we will try and edit that report using this functionallity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xRJLzshqLpVJ"
   },
   "outputs": [],
   "source": [
    "prompt_str = \"give me a small report about {topic}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lfl_s4VALpVJ"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WSMlRM8wLpVJ",
    "outputId": "2b25fd0b-8e2c-4513-9980-25f587eee58b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Report on Artificial Intelligence (AI)\n",
       "\n",
       "#### Introduction\n",
       "Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, particularly computer systems. These processes include learning (the acquisition of information and rules for using it), reasoning (using rules to reach approximate or definite conclusions), and self-correction. AI has become a transformative technology across various sectors, influencing how we work, communicate, and solve problems.\n",
       "\n",
       "#### Types of AI\n",
       "AI can be categorized into two main types:\n",
       "\n",
       "1. **Narrow AI (Weak AI)**: This type of AI is designed and trained for a specific task. Examples include virtual assistants like Siri and Alexa, recommendation systems on platforms like Netflix and Amazon, and image recognition software.\n",
       "\n",
       "2. **General AI (Strong AI)**: This is a theoretical form of AI that possesses the ability to understand, learn, and apply intelligence across a wide range of tasks, similar to a human being. As of now, General AI remains largely conceptual and has not yet been realized.\n",
       "\n",
       "#### Applications of AI\n",
       "AI technologies are being applied in numerous fields, including:\n",
       "\n",
       "- **Healthcare**: AI is used for diagnostic purposes, personalized medicine, and predictive analytics to improve patient outcomes.\n",
       "- **Finance**: Algorithms analyze market trends, assess risks, and automate trading processes.\n",
       "- **Transportation**: AI powers autonomous vehicles, optimizing routes and improving safety.\n",
       "- **Manufacturing**: Robotics and AI enhance production efficiency, predictive maintenance, and quality control.\n",
       "- **Customer Service**: Chatbots and virtual assistants provide 24/7 support, improving customer engagement and satisfaction.\n",
       "\n",
       "#### Challenges and Ethical Considerations\n",
       "Despite its potential, AI poses several challenges:\n",
       "\n",
       "- **Bias and Fairness**: AI systems can perpetuate existing biases present in training data, leading to unfair outcomes.\n",
       "- **Privacy Concerns**: The use of AI in data collection raises significant privacy issues, as personal information can be misused.\n",
       "- **Job Displacement**: Automation driven by AI may lead to job losses in certain sectors, necessitating workforce retraining and adaptation.\n",
       "- **Security Risks**: AI can be exploited for malicious purposes, including cyberattacks and misinformation campaigns.\n",
       "\n",
       "#### Future of AI\n",
       "The future of AI is promising, with ongoing advancements in machine learning, natural language processing, and robotics. As AI continues to evolve, it is expected to play a crucial role in addressing global challenges, such as climate change, healthcare accessibility, and education.\n",
       "\n",
       "#### Conclusion\n",
       "Artificial Intelligence is a rapidly advancing field with the potential to revolutionize various aspects of society. While it offers numerous benefits, it also presents significant ethical and practical challenges that must be addressed to ensure its responsible and equitable deployment. As we move forward, collaboration among technologists, policymakers, and ethicists will be essential to harness the full potential of AI while mitigating its risks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"AI\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XE7HOxwaLpVJ"
   },
   "source": [
    "Here we are making two functions, `extract_fact` to pull out the main content of our text and `replace_word` that will replace AI with Skynet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Wc7ws8FkLpVJ"
   },
   "outputs": [],
   "source": [
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "old_word = \"AI\"\n",
    "new_word = \"skynet\"\n",
    "\n",
    "def replace_word(x):\n",
    "    return x.replace(old_word, new_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktHrg5iSLpVK"
   },
   "source": [
    "Lets wrap these functions and see what the output is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_vFeqMY-LpVK"
   },
   "outputs": [],
   "source": [
    "extract_fact_runnable = RunnableLambda(extract_fact)\n",
    "replace_word_runnable = RunnableLambda(replace_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "TDWN-tQzLpVK"
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 950
    },
    "id": "OCODP8vLLpVK",
    "outputId": "7d4c8a62-1b61-42fa-e6c3-f0dbe634f7f5"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Introduction\n",
       "Retrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge sources, thereby improving the accuracy and relevance of generated responses.\n",
       "#### Concept Overview\n",
       "RAG operates on the principle that while generative models like GPT-3 can produce coherent text, they may lack specific factual information or context that is not part of their training data. By integrating a retrieval mechanism, RAG can pull in relevant documents or data from a predefined corpus, which the generative model can then use to inform its output.\n",
       "#### Mechanism\n",
       "1. **Retrieval Component**: When a query is posed, the system first retrieves relevant documents from a large database or knowledge base. This is typically done using techniques such as vector similarity search or traditional keyword-based search.\n",
       "2. **Generation Component**: The retrieved documents are then fed into a generative model, which synthesizes a response based on both the input query and the retrieved information. This allows the model to produce more informed and contextually relevant answers.\n",
       "3. **End-to-End Training**: RAG models can be trained end-to-end, meaning that both the retrieval and generation components can be optimized together, leading to improved performance over time.\n",
       "#### Applications\n",
       "- **Question Answering**: RAG is particularly effective in applications where users seek specific information, such as customer support or educational tools.\n",
       "- **Content Creation**: It can assist in generating articles, reports, or summaries by pulling in relevant data from various sources.\n",
       "- **Conversational Agents**: RAG enhances chatbots and virtual assistants by providing them with up-to-date information and contextually relevant responses.\n",
       "#### Advantages\n",
       "- **Improved Accuracy**: By leveraging external knowledge, RAG can provide more accurate and factually correct information.\n",
       "- **Contextual Relevance**: The integration of retrieval allows for responses that are more tailored to the specific needs of the user.\n",
       "- **Dynamic Knowledge**: RAG systems can be updated with new information without retraining the entire model, making them adaptable to changing knowledge bases.\n",
       "#### Challenges\n",
       "- **Complexity**: The integration of retrieval and generation components adds complexity to the system architecture.\n",
       "- **Latency**: The retrieval process can introduce delays, which may affect the responsiveness of applications.\n",
       "- **Quality of Retrieved Data**: The effectiveness of RAG is heavily dependent on the quality and relevance of the retrieved documents.\n",
       "#### Conclusion\n",
       "Retrieval-Augmented Generation represents a significant advancement in the field of natural language processing. By combining retrieval and generation, RAG systems can produce more accurate, relevant, and context-aware responses. As research and development in this area continue, RAG is poised to play a crucial role in various applications, enhancing user experiences across multiple domains."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHSumR1tLpVK"
   },
   "source": [
    "Those are our `RunnableLambda` functions. It's worth noting that all inputs to these functions are expected to be a SINGLE arguments. If you have a function that accepts multiple arguments, you can input a dictionary with keys, then unpack them inside the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIXKLyQKLpVK"
   },
   "source": [
    "## LCEL `RunnableParallel` and `RunnablePassthrough`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avFhxg_pLpVK"
   },
   "source": [
    "LCEL provides us with various `Runnable` classes that allow us to control the flow of data and execution order through our chains. Two of these are `RunnableParallel` and `RunnablePassthrough`.\n",
    "\n",
    "* `RunnableParallel` — allows us to run multiple `Runnable` instances in parallel. Acting almost as a Y-fork in the chain.\n",
    "\n",
    "* `RunnablePassthrough` — allows us to pass through a variable to the next `Runnable` without modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTsqIAufLpVK"
   },
   "source": [
    "To see these runnables in action, we will create two data sources, each source provides specific information but to answer the question we will need both to fed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zaD678FMLpVK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/02/b59r49t147vcx84q80s5dl840000gn/T/ipykernel_62938/3699332964.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings()\n",
      "/Users/ettyekhon/code/src/petroineos/petroineos-related/langchain-course/.venv/lib/python3.12/site-packages/docarray/helper.py:224: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  \"\"\"\n",
      "/Users/ettyekhon/code/src/petroineos/petroineos-related/langchain-course/.venv/lib/python3.12/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"half the info is here\",\n",
    "        \"DeepSeek-V3 was released in December 2024\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")\n",
    "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"the other half of the info is here\",\n",
    "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ub4pjRrBLpVK"
   },
   "source": [
    "Here you can see the prompt does have three inputs, two for context and one for the question itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "pnxXrw-CLpVK"
   },
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
    "Context:\n",
    "{context_a}\n",
    "{context_b}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "jL3YN1c1LpVK"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt_str),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YchbLoKLpVK"
   },
   "source": [
    "Here we are wrapping our vector stores as retrievers so they can be fitted into one big retrieval variable to be used by the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "xkhFV8-SLpVL"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a, \"context_b\": retriever_b, \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKtORU5xLpVL"
   },
   "source": [
    "The chain we'll be constructing will look something like this:\n",
    "\n",
    "![](https://github.com/aurelio-labs/langchain-course/blob/main/assets/lcel-flow.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "PNLpfS3_LpVL"
   },
   "outputs": [],
   "source": [
    "chain = retrieval | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo8BfprxLpVL"
   },
   "source": [
    "We `invoke` it as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "84gEXUG1LpVL",
    "outputId": "432d6af2-b59c-4dc5-d390-1ecbfc3efbd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The DeepSeek-V3 model, released in December 2024, is a mixture of experts model with 671 billion parameters.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\n",
    "    \"what architecture does the model DeepSeek released in december use?\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcwr16jsLpVL"
   },
   "source": [
    "With that we've seen how we can use `RunnableParallel` and `RunnablePassthrough` to control the flow of data and execution order through our chains.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
